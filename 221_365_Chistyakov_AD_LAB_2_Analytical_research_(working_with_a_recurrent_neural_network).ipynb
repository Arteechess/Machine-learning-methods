{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arteechess/Machine-learning-methods/blob/main/221_365_Chistyakov_AD_LAB_2_Analytical_research_(working_with_a_recurrent_neural_network).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лабораторная работа №2\n",
        "Аналитические исследования (работа с рекуррентной нейронной сетью)\n",
        "\n",
        "ФИО: Чистяков Артем Дмитриевич\n",
        "\n",
        "Группа: 221-365\n",
        "\n",
        "Номер 10 (2700 образцов)"
      ],
      "metadata": {
        "id": "cGqNhGwGuXu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np  # Добавлено: импорт библиотеки numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Загрузка данных\n",
        "try:\n",
        "    df = pd.read_excel(\"IMDB_dataset.xlsx\")\n",
        "    print(\"Файл Excel успешно загружен и прочитан.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Ошибка: Файл IMDB_dataset.xlsx не найден.\")\n",
        "    exit()  # Завершаем выполнение, если файл не найден\n",
        "\n",
        "# Вывод информации о данных\n",
        "print(\"Названия столбцов:\", df.columns.tolist())\n",
        "print(\"\\nПервые 5 строк датафрейма:\")\n",
        "print(df.head())\n",
        "\n",
        "# Предварительная обработка данных\n",
        "df.dropna(inplace=True)  # Удаление строк с пропущенными значениями\n",
        "df = df.drop_duplicates()  # Удаление дубликатов\n",
        "\n",
        "# Очистка текста\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Удаление HTML-тегов\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)  # Удаление не-буквенных символов\n",
        "    text = text.lower()  # Приведение к нижнему регистру\n",
        "    return text\n",
        "\n",
        "df['review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Лемматизация\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['review'] = df['review'].apply(lemmatize_text)\n",
        "\n",
        "# Выборка данных (без балансировки)\n",
        "df = df.sample(n=min(25000, len(df)), random_state=42)  # Сокращаем размер датасета, но не больше, чем есть\n",
        "\n",
        "print(\"Размер данных после предварительной обработки:\", len(df))\n",
        "\n",
        "reviews = df['review'].tolist()\n",
        "sentiments = df['sentiment'].tolist()\n",
        "\n",
        "#Размер выборки\n",
        "sample_size = 2700\n",
        "\n",
        "reviews = reviews[:sample_size]\n",
        "sentiments = sentiments[:sample_size]\n",
        "\n",
        "print(\"Размер выборки reviews:\", len(reviews))\n",
        "print(\"Размер выборки sentiments:\", len(sentiments))\n",
        "\n",
        "# Анализ данных\n",
        "positive_count = sentiments.count('positive')\n",
        "negative_count = sentiments.count('negative')\n",
        "ratio = positive_count / negative_count if negative_count > 0 else float('inf')\n",
        "\n",
        "print(\"\\nБазовая статистика:\")\n",
        "print(\"Количество отзывов в выборке:\", len(reviews))\n",
        "print(\"Количество положительных отзывов:\", positive_count)\n",
        "print(\"Количество отрицательных отзывов:\", negative_count)\n",
        "print(\"Соотношение положительных и отрицательных отзывов:\", round(ratio, 2))\n",
        "\n",
        "# Анализ длин отзывов\n",
        "print(\"\\nАнализ длин отзывов:\")\n",
        "review_lengths = [len(review) for review in reviews]\n",
        "avg_length = np.mean(review_lengths)\n",
        "max_length = np.max(review_lengths)\n",
        "min_length = np.min(review_lengths) # Исправлено: np.min вместо np.mean\n",
        "print(f\"Средняя длина отзыва: {avg_length}\")\n",
        "print(f\"Максимальная длина отзыва: {max_length}\")\n",
        "print(f\"Минимальная длина отзыва: {min_length}\")\n",
        "\n"
      ],
      
